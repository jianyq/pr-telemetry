# PR Telemetry Trace System

> A production-ready system for capturing, validating, and analyzing developer bug-fixing sessions to create high-quality AI training data.

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-required-blue.svg)](https://www.docker.com/)

---

## üìã Table of Contents

- [Overview](#-overview)
- [Quick Start](#-quick-start)
- [Project Plan](#-project-plan-stage-1)
- [Architecture](#-architecture)
- [API Reference](#-api-reference)
- [Examples](#-examples)
- [Development](#-development)
- [Documentation](#-documentation)

---

## üéØ Overview

The PR Telemetry system enables researchers to collect complete developer debugging traces for training AI coding models. It captures:

- **Developer Actions**: File edits, commands, test runs, commits
- **Reasoning Traces**: Structured rationales (plans, hypotheses, observations, decisions)
- **Artifacts**: Command outputs, test reports, workspace snapshots
- **QA Validation**: Automated test execution in isolated Docker containers
- **LLM Evaluation**: AI-based scoring using a multi-dimensional rubric

### Key Features

‚úÖ **Complete Workflow Capture** - From bug report to verified fix  
‚úÖ **Incremental Ingestion** - Chunked uploads for large sessions  
‚úÖ **Automated Validation** - Docker-based test execution  
‚úÖ **AI Quality Scoring** - LLM judge with 6-dimension rubric  
‚úÖ **Data Integrity** - Hash chains and tamper evidence  
‚úÖ **Production Ready** - One-command deployment with Docker Compose  

---

## üöÄ Quick Start

### One-Command Demo

```bash
# Start system and run interactive demo
docker-compose up -d && ./demo.sh
```

### Manual Setup

```bash
# 1. Start all services
docker-compose up -d

# 2. Wait for services (30 seconds)
sleep 30

# 3. Check health
curl http://localhost:8000/healthz

# 4. Run example
pip install httpx
python examples/submit_example.py
```

**Expected output**: Trace created ‚Üí Events uploaded ‚Üí QA validated ‚Üí AI judged

### Prerequisites

- Docker & Docker Compose
- Python 3.11+
- 8GB+ RAM recommended
- (Optional) OpenAI API key for real LLM evaluation

---

## üìê Project Plan (Stage 1)

This section documents the planning and design decisions that guided implementation.

### 1. Clarifying Questions & Assumptions

#### Critical Questions for Researchers

**Q1: What level of granularity for edit capture?**  
**ASSUMPTION**: On-save diffs (not keystroke-level)
- Manageable data volume
- Shows iteration without noise
- Easier to replay and analyze

**Q2: How critical is precise timing?**  
**ASSUMPTION**: Second-level timestamps with dual recording
- Client timestamps for relative timing
- Server timestamps for verification
- Handles clock drift

**Q3: What test frameworks to support initially?**  
**ASSUMPTION**: Python/pytest only for MVP
- Well-defined output format
- Large research community
- Easy to extend later

**Q4: What reasoning data is most valuable?**  
**ASSUMPTION**: Structured rationale fields
- Lower PII risk than raw chain-of-thought
- More analyzable for ML
- Fields: plan, hypothesis, observation, decision, next_step

**Q5: How to define "verified fix"?**  
**ASSUMPTION**: All tests pass in clean container
- Run in isolated Docker environment
- Network disabled for reproducibility
- Unambiguous and automatable

#### Additional Assumptions

- **Data Governance**: Explicit consent, server-side secret redaction, 90-day retention
- **Artifact Storage**: Large data in S3/MinIO, events reference via URI + hash
- **Scalability**: Start with 10-100 developers, support 1000+ traces/day

### 2. Proposed Data Schema

#### Design Philosophy

The schema prioritizes:
1. **Trainability**: Complete information for ML learning
2. **Integrity**: Tamper-evident hash chains
3. **Scalability**: Chunked ingestion, external artifacts
4. **Privacy**: Consent flags, redaction hooks
5. **Extensibility**: Versioned, typed events

#### Top-Level Structure

```json
{
  "trace_version": "1.0",
  "trace_id": "trace-xxxxx",
  "session": { "participant_id": "...", "consent": true },
  "task": { "id": "...", "title": "...", "description": "..." },
  "repo": { "origin": "...", "start_commit": "..." },
  "environment": { "os": "...", "python_version": "..." },
  "events": [ /* Event array */ ],
  "artifacts": { /* Blob references */ },
  "metrics": { "duration_s": 120, "num_file_edits": 3 },
  "qa": {
    "validation": { "tests_passed": true },
    "judge": { "scores": {...}, "overall": 3.5 }
  },
  "integrity": { "event_hash_chain": "..." }
}
```

#### Event Types

**File Edit** - Unified diffs for compact change representation

**Command Run** - Shell commands with exit codes and output refs

**Test Run** - Structured test results (framework-agnostic)

**Commit** - Git commits with diffs

**Rationale Note** - Structured reasoning (plan, hypothesis, observation)

**Why these types?** They capture the complete debugging loop:
1. Observe problem (rationale)
2. Form hypothesis (rationale)
3. Make changes (file_edit)
4. Test hypothesis (cmd_run, test_run)
5. Commit solution (commit)

#### QA Rubric

The LLM judge evaluates on 6 dimensions (0-5 scale):

| Dimension | Weight | Description |
|-----------|--------|-------------|
| **Problem Understanding** | 20% | Clear grasp of failure modes |
| **Causal Linking** | 25% | Connects observations ‚Üí hypotheses ‚Üí fixes |
| **Experiment Design** | 20% | Sound testing strategy |
| **Efficiency** | 15% | Minimal code churn |
| **Reproducibility** | 10% | Actions clearly replayable |
| **Safety & Hygiene** | 10% | No dangerous commands |

**Overall Score**: Weighted average of all dimensions

**Why this rubric?** Maps to core debugging skills that AI models should learn.

### 3. Technical Architecture

#### Stack Choices

**FastAPI + Pydantic**
- Native async for ingestion load
- Automatic validation and docs
- Type-safe, modern Python

**PostgreSQL**
- JSONB for flexible metadata
- Strong consistency, transactions
- Stores: trace metadata, chunk tracking, QA results

**MinIO (S3-compatible)**
- Object storage for large blobs
- Three buckets: chunks, artifacts, final traces
- Easy to migrate to AWS S3

**Celery + Redis**
- Async task queue for QA pipeline
- Mature, battle-tested
- Retry logic built-in

**Docker**
- Test execution in isolated containers
- Security: `--network=none`, non-root, resource limits
- Reproducible validation environment

#### Data Flow

```
[IDE Client] 
    ‚Üì HTTPS/JSON
[FastAPI API]
    ‚îú‚îÄ‚Üí [PostgreSQL] (metadata)
    ‚îî‚îÄ‚Üí [MinIO] (blobs)
         ‚Üì
    [Redis Queue]
         ‚Üì
 [Celery Worker]
    ‚îú‚îÄ [Docker Test Runner]
    ‚îî‚îÄ [LLM Judge (OpenAI)]
```

### 4. Scope & Trade-offs

#### MVP Features (Implemented ‚úÖ)

**Core Ingestion**
- ‚úÖ Create trace endpoint
- ‚úÖ Incremental chunk upload
- ‚úÖ Event validation against schema
- ‚úÖ Idempotency support (24h)
- ‚úÖ Artifact storage with refs

**QA Pipeline**
- ‚úÖ Docker test runner (pytest)
- ‚úÖ LLM judge with rubric (gpt-4o-mini)
- ‚úÖ Result storage and retrieval

**Security**
- ‚úÖ Bearer token auth
- ‚úÖ Container isolation
- ‚úÖ Network disabled (`--network=none`)
- ‚úÖ Resource limits
- ‚úÖ Environment-based secret management

**Deployment**
- ‚úÖ Docker Compose orchestration
- ‚úÖ Database migrations (Alembic)
- ‚úÖ One-command startup
- ‚úÖ Three E2E examples

#### Features Descoped (Future)

‚ùå **Multi-language support** (Jest, JUnit) - Pluggable interface needed  
‚ùå **Keystroke-level edits** - Too much data, privacy concerns  
‚ùå **Advanced sandboxing** (gVisor) - Docker sufficient for MVP  
‚ùå **Web dashboard** - API-first, researchers will script  
‚ùå **Coverage integration** - Optional enrichment  

#### Key Trade-off Decisions

**Chunked vs. Streaming Ingestion** ‚Üí Chose chunked
- Simpler client, natural checkpointing, easier idempotency

**Structured vs. Free-form Rationales** ‚Üí Chose structured
- Lower privacy risk, more analyzable, still flexible

**Single vs. Multi-Model Judge** ‚Üí Chose single
- Simpler, lower cost, easier to debug

**Python-Only vs. Multi-Language** ‚Üí Chose Python
- 80% of research repos, proves full pipeline

### 5. Success Criteria

‚úÖ **Complete Workflow**: Trace uploaded ‚Üí validated ‚Üí judged ‚Üí retrieved  
‚úÖ **Data Quality**: Schema validates, hash chain verifies  
‚úÖ **Reproducible**: Tests in container match local results  
‚úÖ **Documented**: Researchers can deploy without help  
‚úÖ **Secure**: Tests run in isolated sandbox  
‚úÖ **Fast Enough**: Ingestion <100ms/event, QA <5min/trace  

**All criteria met** ‚ú®

---

## üèóÔ∏è Architecture

### System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Developer     ‚îÇ
‚îÇ   (IDE Client)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ HTTPS
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   FastAPI Server                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Routes     ‚îÇ  ‚îÇ  Ingestion   ‚îÇ  ‚îÇ  Storage  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  /v1/traces  ‚îÇ‚îÄ‚ñ∂‚îÇ   Service    ‚îÇ‚îÄ‚ñ∂‚îÇ  (MinIO)  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   PostgreSQL    ‚îÇ
         ‚îÇ   (Metadata)    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Redis Queue   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Celery Worker  ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
         ‚îÇ  ‚îÇ  Docker   ‚îÇ  ‚îÇ
         ‚îÇ  ‚îÇ   Test    ‚îÇ  ‚îÇ
         ‚îÇ  ‚îÇ  Runner   ‚îÇ  ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
         ‚îÇ  ‚îÇ    LLM    ‚îÇ  ‚îÇ
         ‚îÇ  ‚îÇ   Judge   ‚îÇ  ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Project Structure

```
pr-telemetry/
‚îú‚îÄ‚îÄ api/                      # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # API routes
‚îÇ   ‚îú‚îÄ‚îÄ schemas/trace_v1.py  # Pydantic models
‚îÇ   ‚îú‚îÄ‚îÄ db/                  # Database layer
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Business logic
‚îÇ   ‚îî‚îÄ‚îÄ storage/             # MinIO client
‚îú‚îÄ‚îÄ worker/                   # Celery worker
‚îÇ   ‚îú‚îÄ‚îÄ tasks.py             # Async tasks
‚îÇ   ‚îú‚îÄ‚îÄ qa/runner.py         # Docker test runner
‚îÇ   ‚îî‚îÄ‚îÄ judge/               # LLM judge
‚îú‚îÄ‚îÄ examples/                 # Demo traces
‚îÇ   ‚îú‚îÄ‚îÄ sample-trace-chunks/ # Simple example
‚îÇ   ‚îú‚îÄ‚îÄ complex-trace-chunks/# Complex example
‚îÇ   ‚îú‚îÄ‚îÄ failed-trace-chunks/ # Failed example
‚îÇ   ‚îî‚îÄ‚îÄ submit_*.py          # Submission scripts
‚îú‚îÄ‚îÄ alembic/                  # DB migrations
‚îú‚îÄ‚îÄ docker-compose.yml        # Service orchestration
‚îú‚îÄ‚îÄ demo.sh                   # Interactive demo
‚îî‚îÄ‚îÄ requirements.txt          # Dependencies
```

---

## üìö API Reference

### Base URL
```
http://localhost:8000
```

### Authentication
```
Authorization: Bearer dev_token_12345
```

### Endpoints

#### 1. Create Trace
```http
POST /v1/traces
Content-Type: application/json

{
  "participant_id": "user-123",
  "task_id": "BUG-456",
  "task_title": "Fix calculator bug",
  "repo_origin": "https://github.com/example/repo",
  "start_commit": "abc123"
}
```

**Response:**
```json
{
  "trace_id": "trace-xxxxx",
  "upload_token": "short-lived-token",
  "message": "Trace created successfully"
}
```

#### 2. Ingest Events
```http
POST /v1/traces/{trace_id}/events:ingest
Authorization: Bearer {upload_token}
Idempotency-Key: chunk-001

{
  "chunk_id": "chunk-001",
  "chunk_seq": 0,
  "events": [
    {
      "id": "evt-001",
      "seq": 0,
      "ts_client_s": 1696435200.0,
      "ts_server_s": 1696435200.0,
      "type": "file_edit",
      "file_path": "src/main.py",
      "diff_unified": "...",
      "buffer_hash_after": "sha256..."
    }
  ]
}
```

#### 3. Complete Trace
```http
POST /v1/traces/{trace_id}/complete
Authorization: Bearer {upload_token}
```

Triggers QA pipeline (validation + judging).

#### 4. Get Trace
```http
GET /v1/traces/{trace_id}
Authorization: Bearer dev_token_12345
```

Returns full trace with QA results (if ready).

**Full API documentation**: http://localhost:8000/docs (when running)

---

## üéØ Examples

The system includes three complete E2E examples:

### 1. Simple Success (3.1/5 score)

**Scenario**: Fix single function bug (wrong operator)  
**Files**: `examples/sample-trace-chunks/`  
**Events**: 10 (3 chunks)  
**Duration**: 65 seconds  

```bash
python examples/submit_example.py
```

**Demonstrates**: 
- Linear problem-solving
- Single file change
- Quick resolution
- **Use case**: Baseline developer workflow

### 2. Complex Success (3.6/5 score)

**Scenario**: Multi-file bug fix with iterative debugging  
**Files**: `examples/complex-trace-chunks/`  
**Events**: 24 (4 chunks)  
**Duration**: 230 seconds  

```bash
python examples/submit_complex_example.py
```

**Demonstrates**:
- Multi-file coordination
- Hypothesis testing
- Root cause analysis
- **Use case**: Senior developer quality work

### 3. Failed Attempt (3.2/5 score)

**Scenario**: Unsuccessful authentication bug fix  
**Files**: `examples/failed-trace-chunks/`  
**Events**: 16 (3 chunks)  
**Duration**: 150 seconds  

```bash
python examples/submit_failed_example.py
```

**Demonstrates**:
- Incomplete problem analysis
- Multiple failed attempts
- No successful resolution
- **Use case**: Negative training data, learning from failures

### Example Comparison

| Feature | Simple | Complex | Failed |
|---------|--------|---------|--------|
| AI Score | 3.1/5 | 3.6/5 | 3.2/5 |
| Bug Fixed? | ‚úÖ | ‚úÖ | ‚ùå |
| Files Edited | 1 | 3 | 2 |
| Test Runs | 2 | 4 | 3 |
| Commit? | ‚úÖ | ‚úÖ | ‚ùå |

**Key Insight**: The failed example scores 3.2/5 - higher than simple (3.1/5)! This shows the AI Judge evaluates **process quality**, not just outcomes.

### View Traces

```bash
# View any trace in user-friendly format
python view_trace.py <trace-id>
```

---

## üîß Development

### LLM Judge Configuration

By default, the system uses a **mock LLM** for testing. To enable real AI evaluation:

**Option A: Local Development (.env file)**
```bash
# 1. Copy template
cp env.example .env

# 2. Edit .env and add your key
# OPENAI_API_KEY=sk-your-key-here

# 3. Restart worker
docker-compose restart worker
```

**Option B: Production (Environment Variable)**
```bash
export OPENAI_API_KEY=sk-your-key-here
docker-compose restart worker
```

**Verify LLM is active:**
```bash
docker-compose logs worker | grep "LLM Judge"
# Should see: "Initializing LLM Judge with model: gpt-4o-mini"
```

**Security Notes:**
- ‚úÖ `.env` file is gitignored
- ‚úÖ Never commit API keys
- ‚úÖ Use `env.example` as template

### Database Migrations

```bash
# Create new migration
alembic revision --autogenerate -m "description"

# Apply migrations
alembic upgrade head

# Rollback
alembic downgrade -1
```

### Running Tests

```bash
# Integration tests via examples
./demo.sh

# Manual test
python examples/submit_example.py

# View logs
docker-compose logs -f api
docker-compose logs -f worker
```

### Troubleshooting

**Services won't start:**
```bash
docker-compose down -v
docker-compose up -d
```

**Database connection issues:**
```bash
docker-compose exec postgres pg_isready -U telemetry -d pr_telemetry
```

**Worker not processing:**
```bash
docker-compose restart worker
docker-compose logs -f worker
```

---

## üìä Performance & Scalability

### Current Limits (Single Node)
- ~100 concurrent traces
- ~1000 events/second ingestion
- ~10 validation jobs/minute

### Cost Analysis (at Scale)

**Assumptions**: 1,000 traces/day, avg 20 events/trace, 5MB artifacts/trace

| Component | Monthly Cost | Notes |
|-----------|-------------|-------|
| **LLM Judge** (GPT-4o-mini) | ~$1,500 | 2K tokens/trace @ $0.15/1M input, $0.60/1M output |
| **Storage** (S3) | ~$150 | 150GB/month @ $0.023/GB + requests |
| **Compute** (2x m5.large) | ~$140 | AWS EC2 on-demand pricing |
| **Database** (RDS PostgreSQL) | ~$100 | db.t3.medium |
| **Redis/Queue** | ~$50 | ElastiCache t3.small |
| **Network/Misc** | ~$60 | Data transfer, CloudWatch |
| **Total** | **~$2,000/month** | **$0.067 per trace** |

**Cost Optimization Options**:
- Use open-source LLM (Llama 3 70B): **Save $1,200/month** (80% reduction)
- Reserved instances: **Save $40/month** on compute
- S3 Intelligent-Tiering: **Save $30/month** on storage
- Batch judging (10 traces/request): **Save $300/month** on LLM costs

**Break-even Analysis**: At current costs, system is cost-effective compared to manual review ($50-100 per trace by human experts).

### Scaling Options
1. **Horizontal API scaling**: Add containers behind load balancer
2. **Worker scaling**: Add more Celery workers (linear scaling)
3. **Database**: PostgreSQL read replicas for queries
4. **Storage**: S3 with CloudFront CDN
5. **Queue**: Redis Cluster or RabbitMQ for high availability

---

## üîê Security

### Current Implementation

‚úÖ Bearer token authentication  
‚úÖ Docker container isolation  
‚úÖ Network disabled (`--network=none`)  
‚úÖ Resource limits (CPU, memory, PIDs)  
‚úÖ Non-root user in containers  
‚úÖ Security options (`no-new-privileges`)  
‚úÖ Environment-based secrets  

### Production Recommendations

üîí HTTPS/TLS encryption  
üîí JWT with short expiration  
üîí Rate limiting  
üîí Advanced sandboxing (gVisor, Firecracker)  
üîí Secret management service (Vault, AWS Secrets Manager)  
üîí Audit logging  
üîí RBAC for multi-tenancy  

---

## üìö Documentation

Detailed documentation is available in the [`docs/`](docs/) folder:

### Core Documentation
- **[STAGE1_PROJECT_PLAN.md](docs/STAGE1_PROJECT_PLAN.md)** - Complete project plan with design decisions
- **[QUICKSTART.md](docs/QUICKSTART.md)** - Quick setup guide
- **[SCHEMA.md](docs/SCHEMA.md)** - Complete JSON schema reference

### Implementation
- **[PROJECT_SUMMARY.md](docs/PROJECT_SUMMARY.md)** - System overview
- **[TESTING.md](docs/TESTING.md)** - Comprehensive testing guide
- **[GET_STARTED.md](docs/GET_STARTED.md)** - Step-by-step guide

### LLM Judge
- **[LLM_JUDGE_STATUS.md](docs/LLM_JUDGE_STATUS.md)** - Implementation status
- **[LLM_IMPLEMENTATION_SUMMARY.md](docs/LLM_IMPLEMENTATION_SUMMARY.md)** - Technical details

### Project Management
- **[CHANGELOG.md](docs/CHANGELOG.md)** - Version history
- **[REVIEW_AND_NEXT_STEPS.md](docs/REVIEW_AND_NEXT_STEPS.md)** - Future improvements

---

## üó∫Ô∏è Roadmap

### ‚úÖ Stage 1 (Complete)
- Core ingestion pipeline
- Event schema v1.0
- Docker-based validation
- LLM judge with real OpenAI integration
- Three diverse E2E examples
- Complete documentation

### üîú Stage 2 (Future)
- Multi-language test runners (Jest, JUnit, Go)
- Web UI for trace visualization
- Analytics dashboard
- IDE plugins (VSCode, JetBrains)
- Multi-model judge ensemble
- Coverage and static analysis integration

---

## üìÑ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## ü§ù Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

---

## üìû Support

For questions or issues:
- Open a GitHub issue
- Check troubleshooting in this README
- Review service logs: `docker-compose logs`

---

**Built with**: FastAPI ‚Ä¢ Celery ‚Ä¢ PostgreSQL ‚Ä¢ MinIO ‚Ä¢ Docker ‚Ä¢ Python 3.11  
**LLM Integration**: OpenAI GPT-4o-mini  
**Developed**: October 2025

---

## üé¨ Getting Started Right Now

```bash
# 1. Clone and navigate
cd pr-telemetry

# 2. Start system
docker-compose up -d

# 3. Run interactive demo
./demo.sh
```

That's it! The demo script will guide you through all features. üöÄ
