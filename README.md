# PR Telemetry Trace System

> A production-ready system for capturing, validating, and analyzing developer bug-fixing sessions to create high-quality AI training data.

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-required-blue.svg)](https://www.docker.com/)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Quick Start](#-quick-start)
- [Project Plan](#-project-plan-stage-1)
- [Architecture](#-architecture)
- [API Reference](#-api-reference)
- [Examples](#-examples)
- [Development](#-development)
- [Documentation](#-documentation)

---

## ğŸ¯ Overview

The PR Telemetry system enables researchers to collect complete developer debugging traces for training AI coding models. It captures:

- **Developer Actions**: File edits, commands, test runs, commits
- **Reasoning Traces**: Structured rationales (plans, hypotheses, observations, decisions)
- **Artifacts**: Command outputs, test reports, workspace snapshots
- **QA Validation**: Automated test execution in isolated Docker containers
- **LLM Evaluation**: AI-based scoring using a multi-dimensional rubric

### Key Features

âœ… **Complete Workflow Capture** - From bug report to verified fix  
âœ… **Incremental Ingestion** - Chunked uploads for large sessions  
âœ… **Automated Validation** - Docker-based test execution  
âœ… **AI Quality Scoring** - LLM judge with 6-dimension rubric  
âœ… **Data Integrity** - Hash chains and tamper evidence  
âœ… **Production Ready** - One-command deployment with Docker Compose  

---

## ğŸš€ Quick Start

### One-Command Demo

```bash
# Start system and run interactive demo
docker-compose up -d && ./demo.sh
```

### Manual Setup

```bash
# 1. Start all services
docker-compose up -d

# 2. Wait for services (30 seconds)
sleep 30

# 3. Check health
curl http://localhost:8000/healthz

# 4. Run example
pip install httpx
python examples/submit_example.py
```

**Expected output**: Trace created â†’ Events uploaded â†’ QA validated â†’ AI judged

### Prerequisites

- Docker & Docker Compose
- Python 3.11+
- 8GB+ RAM recommended
- (Optional) OpenAI API key for real LLM evaluation

---

## ğŸ“ Project Plan (Stage 1)

This section documents the planning and design decisions that guided implementation.

### 1. Clarifying Questions & Assumptions

#### Critical Questions for Researchers

**Q1: What level of granularity for edit capture?**  
**ASSUMPTION**: On-save diffs (not keystroke-level)
- Manageable data volume
- Shows iteration without noise
- Easier to replay and analyze

**Q2: How critical is precise timing?**  
**ASSUMPTION**: Second-level timestamps with dual recording
- Client timestamps for relative timing
- Server timestamps for verification
- Handles clock drift

**Q3: What test frameworks to support initially?**  
**ASSUMPTION**: Python/pytest only for MVP
- Well-defined output format
- Large research community
- Easy to extend later

**Q4: What reasoning data is most valuable?**  
**ASSUMPTION**: Structured rationale fields
- Lower PII risk than raw chain-of-thought
- More analyzable for ML
- Fields: plan, hypothesis, observation, decision, next_step

**Q5: How to define "verified fix"?**  
**ASSUMPTION**: All tests pass in clean container
- Run in isolated Docker environment
- Network disabled for reproducibility
- Unambiguous and automatable

#### Additional Assumptions

- **Data Governance**: Explicit consent, server-side secret redaction, 90-day retention
- **Artifact Storage**: Large data in S3/MinIO, events reference via URI + hash
- **Scalability**: Start with 10-100 developers, support 1000+ traces/day

### 2. Proposed Data Schema

#### Design Philosophy

The schema prioritizes:
1. **Trainability**: Complete information for ML learning
2. **Integrity**: Tamper-evident hash chains
3. **Scalability**: Chunked ingestion, external artifacts
4. **Privacy**: Consent flags, redaction hooks
5. **Extensibility**: Versioned, typed events

#### Top-Level Structure

```json
{
  "trace_version": "1.0",
  "trace_id": "trace-xxxxx",
  "session": { "participant_id": "...", "consent": true },
  "task": { "id": "...", "title": "...", "description": "..." },
  "repo": { "origin": "...", "start_commit": "..." },
  "environment": { "os": "...", "python_version": "..." },
  "events": [ /* Event array */ ],
  "artifacts": { /* Blob references */ },
  "metrics": { "duration_s": 120, "num_file_edits": 3 },
  "qa": {
    "validation": { "tests_passed": true },
    "judge": { "scores": {...}, "overall": 3.5 }
  },
  "integrity": { "event_hash_chain": "..." }
}
```

#### Event Types

**File Edit** - Unified diffs for compact change representation

**Command Run** - Shell commands with exit codes and output refs

**Test Run** - Structured test results (framework-agnostic)

**Commit** - Git commits with diffs

**Rationale Note** - Structured reasoning (plan, hypothesis, observation)

**Why these types?** They capture the complete debugging loop:
1. Observe problem (rationale)
2. Form hypothesis (rationale)
3. Make changes (file_edit)
4. Test hypothesis (cmd_run, test_run)
5. Commit solution (commit)

#### QA Rubric

The LLM judge evaluates on 6 dimensions (0-5 scale):

| Dimension | Weight | Description |
|-----------|--------|-------------|
| **Problem Understanding** | 20% | Clear grasp of failure modes |
| **Causal Linking** | 25% | Connects observations â†’ hypotheses â†’ fixes |
| **Experiment Design** | 20% | Sound testing strategy |
| **Efficiency** | 15% | Minimal code churn |
| **Reproducibility** | 10% | Actions clearly replayable |
| **Safety & Hygiene** | 10% | No dangerous commands |

**Overall Score**: Weighted average of all dimensions

**Why this rubric?** Maps to core debugging skills that AI models should learn.

### 3. Technical Architecture

#### Stack Choices

**FastAPI + Pydantic**
- Native async for ingestion load
- Automatic validation and docs
- Type-safe, modern Python

**PostgreSQL**
- JSONB for flexible metadata
- Strong consistency, transactions
- Stores: trace metadata, chunk tracking, QA results

**MinIO (S3-compatible)**
- Object storage for large blobs
- Three buckets: chunks, artifacts, final traces
- Easy to migrate to AWS S3

**Celery + Redis**
- Async task queue for QA pipeline
- Mature, battle-tested
- Retry logic built-in

**Docker**
- Test execution in isolated containers
- Security: `--network=none`, non-root, resource limits
- Reproducible validation environment

#### Data Flow

```
[IDE Client] 
    â†“ HTTPS/JSON
[FastAPI API]
    â”œâ”€â†’ [PostgreSQL] (metadata)
    â””â”€â†’ [MinIO] (blobs)
         â†“
    [Redis Queue]
         â†“
 [Celery Worker]
    â”œâ”€ [Docker Test Runner]
    â””â”€ [LLM Judge (OpenAI)]
```

### 4. Scope & Trade-offs

#### MVP Features (Implemented âœ…)

**Core Ingestion**
- âœ… Create trace endpoint
- âœ… Incremental chunk upload
- âœ… Event validation against schema
- âœ… Idempotency support (24h)
- âœ… Artifact storage with refs

**QA Pipeline**
- âœ… Docker test runner (pytest)
- âœ… LLM judge with rubric (gpt-4o-mini)
- âœ… Result storage and retrieval

**Security**
- âœ… Bearer token auth
- âœ… Container isolation
- âœ… Network disabled (`--network=none`)
- âœ… Resource limits
- âœ… Environment-based secret management

**Deployment**
- âœ… Docker Compose orchestration
- âœ… Database migrations (Alembic)
- âœ… One-command startup
- âœ… Three E2E examples

#### Features Descoped (Future)

âŒ **Multi-language support** (Jest, JUnit) - Pluggable interface needed  
âŒ **Keystroke-level edits** - Too much data, privacy concerns  
âŒ **Advanced sandboxing** (gVisor) - Docker sufficient for MVP  
âŒ **Web dashboard** - API-first, researchers will script  
âŒ **Coverage integration** - Optional enrichment  

#### Key Trade-off Decisions

**Chunked vs. Streaming Ingestion** â†’ Chose chunked
- Simpler client, natural checkpointing, easier idempotency

**Structured vs. Free-form Rationales** â†’ Chose structured
- Lower privacy risk, more analyzable, still flexible

**Single vs. Multi-Model Judge** â†’ Chose single
- Simpler, lower cost, easier to debug

**Python-Only vs. Multi-Language** â†’ Chose Python
- 80% of research repos, proves full pipeline

### 5. Success Criteria

âœ… **Complete Workflow**: Trace uploaded â†’ validated â†’ judged â†’ retrieved  
âœ… **Data Quality**: Schema validates, hash chain verifies  
âœ… **Reproducible**: Tests in container match local results  
âœ… **Documented**: Researchers can deploy without help  
âœ… **Secure**: Tests run in isolated sandbox  
âœ… **Fast Enough**: Ingestion <100ms/event, QA <5min/trace  

**All criteria met** âœ¨

---

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Developer     â”‚
â”‚   (IDE Client)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTPS
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Server                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Routes     â”‚  â”‚  Ingestion   â”‚  â”‚  Storage  â”‚ â”‚
â”‚  â”‚  /v1/traces  â”‚â”€â–¶â”‚   Service    â”‚â”€â–¶â”‚  (MinIO)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   PostgreSQL    â”‚
         â”‚   (Metadata)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Redis Queue   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Celery Worker  â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚  Docker   â”‚  â”‚
         â”‚  â”‚   Test    â”‚  â”‚
         â”‚  â”‚  Runner   â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â”‚  â”‚    LLM    â”‚  â”‚
         â”‚  â”‚   Judge   â”‚  â”‚
         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Project Structure

```
pr-telemetry/
â”œâ”€â”€ api/                      # FastAPI application
â”‚   â”œâ”€â”€ main.py              # API routes
â”‚   â”œâ”€â”€ schemas/trace_v1.py  # Pydantic models
â”‚   â”œâ”€â”€ db/                  # Database layer
â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â””â”€â”€ storage/             # MinIO client
â”œâ”€â”€ worker/                   # Celery worker
â”‚   â”œâ”€â”€ tasks.py             # Async tasks
â”‚   â”œâ”€â”€ qa/runner.py         # Docker test runner
â”‚   â””â”€â”€ judge/               # LLM judge
â”œâ”€â”€ examples/                 # Demo traces
â”‚   â”œâ”€â”€ sample-trace-chunks/ # Simple example
â”‚   â”œâ”€â”€ complex-trace-chunks/# Complex example
â”‚   â”œâ”€â”€ failed-trace-chunks/ # Failed example
â”‚   â””â”€â”€ submit_*.py          # Submission scripts
â”œâ”€â”€ alembic/                  # DB migrations
â”œâ”€â”€ docker-compose.yml        # Service orchestration
â”œâ”€â”€ demo.sh                   # Interactive demo
â””â”€â”€ requirements.txt          # Dependencies
```

---

## ğŸ“š API Reference

### Base URL
```
http://localhost:8000
```

### Authentication
```
Authorization: Bearer dev_token_12345
```

### Endpoints

#### 1. Create Trace
```http
POST /v1/traces
Content-Type: application/json

{
  "participant_id": "user-123",
  "task_id": "BUG-456",
  "task_title": "Fix calculator bug",
  "repo_origin": "https://github.com/example/repo",
  "start_commit": "abc123"
}
```

**Response:**
```json
{
  "trace_id": "trace-xxxxx",
  "upload_token": "short-lived-token",
  "message": "Trace created successfully"
}
```

#### 2. Ingest Events
```http
POST /v1/traces/{trace_id}/events:ingest
Authorization: Bearer {upload_token}
Idempotency-Key: chunk-001

{
  "chunk_id": "chunk-001",
  "chunk_seq": 0,
  "events": [
    {
      "id": "evt-001",
      "seq": 0,
      "ts_client_s": 1696435200.0,
      "ts_server_s": 1696435200.0,
      "type": "file_edit",
      "file_path": "src/main.py",
      "diff_unified": "...",
      "buffer_hash_after": "sha256..."
    }
  ]
}
```

#### 3. Complete Trace
```http
POST /v1/traces/{trace_id}/complete
Authorization: Bearer {upload_token}
```

Triggers QA pipeline (validation + judging).

#### 4. Get Trace
```http
GET /v1/traces/{trace_id}
Authorization: Bearer dev_token_12345
```

Returns full trace with QA results (if ready).

**Full API documentation**: http://localhost:8000/docs (when running)

---

## ğŸ¯ Examples

The system includes three complete E2E examples:

### 1. Simple Success (3.1/5 score)

**Scenario**: Fix single function bug (wrong operator)  
**Files**: `examples/sample-trace-chunks/`  
**Events**: 10 (3 chunks)  
**Duration**: 65 seconds  

```bash
python examples/submit_example.py
```

**Demonstrates**: 
- Linear problem-solving
- Single file change
- Quick resolution
- **Use case**: Baseline developer workflow

### 2. Complex Success (3.6/5 score)

**Scenario**: Multi-file bug fix with iterative debugging  
**Files**: `examples/complex-trace-chunks/`  
**Events**: 24 (4 chunks)  
**Duration**: 230 seconds  

```bash
python examples/submit_complex_example.py
```

**Demonstrates**:
- Multi-file coordination
- Hypothesis testing
- Root cause analysis
- **Use case**: Senior developer quality work

### 3. Failed Attempt (3.2/5 score)

**Scenario**: Unsuccessful authentication bug fix  
**Files**: `examples/failed-trace-chunks/`  
**Events**: 16 (3 chunks)  
**Duration**: 150 seconds  

```bash
python examples/submit_failed_example.py
```

**Demonstrates**:
- Incomplete problem analysis
- Multiple failed attempts
- No successful resolution
- **Use case**: Negative training data, learning from failures

### Example Comparison

| Feature | Simple | Complex | Failed |
|---------|--------|---------|--------|
| AI Score | 3.1/5 | 3.6/5 | 3.2/5 |
| Bug Fixed? | âœ… | âœ… | âŒ |
| Files Edited | 1 | 3 | 2 |
| Test Runs | 2 | 4 | 3 |
| Commit? | âœ… | âœ… | âŒ |

**Key Insight**: The failed example scores 3.2/5 - higher than simple (3.1/5)! This shows the AI Judge evaluates **process quality**, not just outcomes.

### View Traces

```bash
# View any trace in user-friendly format
python view_trace.py <trace-id>
```

---

## ğŸ”§ Development

### LLM Judge Configuration

By default, the system uses a **mock LLM** for testing. To enable real AI evaluation:

**Option A: Local Development (.env file)**
```bash
# 1. Copy template
cp env.example .env

# 2. Edit .env and add your key
# OPENAI_API_KEY=sk-your-key-here

# 3. Restart worker
docker-compose restart worker
```

**Option B: Production (Environment Variable)**
```bash
export OPENAI_API_KEY=sk-your-key-here
docker-compose restart worker
```

**Verify LLM is active:**
```bash
docker-compose logs worker | grep "LLM Judge"
# Should see: "Initializing LLM Judge with model: gpt-4o-mini"
```

**Security Notes:**
- âœ… `.env` file is gitignored
- âœ… Never commit API keys
- âœ… Use `env.example` as template

### Database Migrations

```bash
# Create new migration
alembic revision --autogenerate -m "description"

# Apply migrations
alembic upgrade head

# Rollback
alembic downgrade -1
```

### Running Tests

```bash
# Integration tests via examples
./demo.sh

# Manual test
python examples/submit_example.py

# View logs
docker-compose logs -f api
docker-compose logs -f worker
```

### Troubleshooting

**Services won't start:**
```bash
docker-compose down -v
docker-compose up -d
```

**Database connection issues:**
```bash
docker-compose exec postgres pg_isready -U telemetry -d pr_telemetry
```

**Worker not processing:**
```bash
docker-compose restart worker
docker-compose logs -f worker
```

---

## ğŸ“Š Performance & Scalability

### Current Limits (Single Node)
- ~100 concurrent traces
- ~1000 events/second ingestion
- ~10 validation jobs/minute

### Scaling Options
1. **Horizontal API scaling**: Add containers behind load balancer
2. **Worker scaling**: Add more Celery workers
3. **Database**: PostgreSQL read replicas
4. **Storage**: Switch to AWS S3
5. **Queue**: Redis Cluster or RabbitMQ

---

## ğŸ” Security

### Current Implementation

âœ… Bearer token authentication  
âœ… Docker container isolation  
âœ… Network disabled (`--network=none`)  
âœ… Resource limits (CPU, memory, PIDs)  
âœ… Non-root user in containers  
âœ… Security options (`no-new-privileges`)  
âœ… Environment-based secrets  

### Production Recommendations

ğŸ”’ HTTPS/TLS encryption  
ğŸ”’ JWT with short expiration  
ğŸ”’ Rate limiting  
ğŸ”’ Advanced sandboxing (gVisor, Firecracker)  
ğŸ”’ Secret management service (Vault, AWS Secrets Manager)  
ğŸ”’ Audit logging  
ğŸ”’ RBAC for multi-tenancy  

---

## ğŸ“š Documentation

Detailed documentation is available in the [`docs/`](docs/) folder:

### Core Documentation
- **[STAGE1_PROJECT_PLAN.md](docs/STAGE1_PROJECT_PLAN.md)** - Complete project plan with design decisions
- **[QUICKSTART.md](docs/QUICKSTART.md)** - Quick setup guide
- **[SCHEMA.md](docs/SCHEMA.md)** - Complete JSON schema reference

### Implementation
- **[PROJECT_SUMMARY.md](docs/PROJECT_SUMMARY.md)** - System overview
- **[TESTING.md](docs/TESTING.md)** - Comprehensive testing guide
- **[GET_STARTED.md](docs/GET_STARTED.md)** - Step-by-step guide

### LLM Judge
- **[LLM_JUDGE_STATUS.md](docs/LLM_JUDGE_STATUS.md)** - Implementation status
- **[LLM_IMPLEMENTATION_SUMMARY.md](docs/LLM_IMPLEMENTATION_SUMMARY.md)** - Technical details

### Project Management
- **[CHANGELOG.md](docs/CHANGELOG.md)** - Version history
- **[REVIEW_AND_NEXT_STEPS.md](docs/REVIEW_AND_NEXT_STEPS.md)** - Future improvements

---

## ğŸ—ºï¸ Roadmap

### âœ… Stage 1 (Complete)
- Core ingestion pipeline
- Event schema v1.0
- Docker-based validation
- LLM judge with real OpenAI integration
- Three diverse E2E examples
- Complete documentation

### ğŸ”œ Stage 2 (Future)
- Multi-language test runners (Jest, JUnit, Go)
- Web UI for trace visualization
- Analytics dashboard
- IDE plugins (VSCode, JetBrains)
- Multi-model judge ensemble
- Coverage and static analysis integration

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

---

## ğŸ“ Support

For questions or issues:
- Open a GitHub issue
- Check troubleshooting in this README
- Review service logs: `docker-compose logs`

---

**Built with**: FastAPI â€¢ Celery â€¢ PostgreSQL â€¢ MinIO â€¢ Docker â€¢ Python 3.11  
**LLM Integration**: OpenAI GPT-4o-mini  
**Developed**: October 2025

---

## ğŸ¬ Getting Started Right Now

```bash
# 1. Clone and navigate
cd pr-telemetry

# 2. Start system
docker-compose up -d

# 3. Run interactive demo
./demo.sh
```

That's it! The demo script will guide you through all features. ğŸš€
